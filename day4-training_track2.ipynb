{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelWithLMHead,\n",
    "    BertForMultipleChoice,\n",
    "    RobertaForMultipleChoice,\n",
    "    XLNetForMultipleChoice,\n",
    "    GPT2LMHeadModel,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from dataset import (\n",
    "    ResponseGenerationDataset,\n",
    "    ResponseGenerationEvalDataset,\n",
    "    KnowledgeSelectionDataset,\n",
    "    SPECIAL_TOKENS\n",
    ")\n",
    "\n",
    "from utils.argument import (\n",
    "    set_default_params,\n",
    "    set_default_dataset_params,\n",
    "    update_additional_params,\n",
    "    verify_args\n",
    ")\n",
    "\n",
    "from utils.data import write_selection_preds, write_detection_preds\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set arguments for dataset, model, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    checkpoint = None,\n",
    "    dataroot = 'data',\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    eval_all_snippets = False,\n",
    "    eval_dataset = 'val',\n",
    "    eval_desc = '',\n",
    "    eval_only = False,\n",
    "    exp_name = 'generation',\n",
    "    history_max_tokens = -1,\n",
    "    knowledge_file = 'knowledge.json',\n",
    "    knowledge_max_tokens = -1,\n",
    "    labels_file = None,\n",
    "    local_rank = -1,\n",
    "    model_name_or_path = 'gpt2',\n",
    "    model_type = '',\n",
    "    negative_sample_method = '',\n",
    "    no_labels = False,\n",
    "    output_dir = \"outputs\",\n",
    "    params_file = \"configs/generation_params.json\",\n",
    "    relevance_ratio = '1',\n",
    "    task_dataset = ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.params_file, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    args = vars(args)\n",
    "\n",
    "    update_additional_params(params, args)\n",
    "    args.update(params)\n",
    "    args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.params = params # used for saving checkpoints\n",
    "set_default_params(args)\n",
    "dataset_args = Namespace(**args.dataset_args)\n",
    "set_default_dataset_params(dataset_args)\n",
    "dataset_args.local_rank = args.local_rank\n",
    "dataset_args.task = args.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# args.n_gpu = torch.cuda.device_count()\n",
    "# args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = 1\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50264, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "### Choose the model ###\n",
    "model = AutoModelWithLMHead.from_pretrained(args.model_name_or_path, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50264, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cb65b9f0e644f49a1375a0bf299701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72518.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd4cf0c2ed24290af8da23cffe4ba48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72518.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eb1379cfdc47b6a57991df9d835bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8646c98501b04d3f98e2ecc4b2847bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ResponseGenerationDataset(dataset_args, tokenizer, split_type=\"train\")\n",
    "eval_dataset = ResponseGenerationDataset(dataset_args, tokenizer, split_type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, eval_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, run_batch_fn_train, run_batch_fn_eval) -> Tuple[int, float]:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        log_dir = os.path.join(\"outputs\", args.exp_name) if args.exp_name else \"outputs\"\n",
    "        tb_writer = SummaryWriter(log_dir)\n",
    "        args.output_dir = log_dir\n",
    "\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=args.train_batch_size,\n",
    "        collate_fn=train_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        0, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # for reproducibility\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        local_steps = 0\n",
    "        tr_loss = 0.0\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            loss, _, _, _ = run_batch_fn_train(args, model, batch)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                local_steps += 1\n",
    "                epoch_iterator.set_postfix(Loss=tr_loss/local_steps)\n",
    "\n",
    "        results = evaluate(args, eval_dataset, model, tokenizer, run_batch_fn_eval, desc=str(global_step))\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            for key, value in results.items():\n",
    "                tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "            tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "            tb_writer.add_scalar(\"loss\", tr_loss / local_steps, global_step)\n",
    "\n",
    "            checkpoint_prefix = \"checkpoint\"\n",
    "            # Save model checkpoint\n",
    "            output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            with open(os.path.join(output_dir, \"params.json\"), \"w\") as jsonfile:\n",
    "                json.dump(args.params, jsonfile, indent=2, default=lambda x: str(x))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / local_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, eval_dataset, model, tokenizer, run_batch_fn, desc=\"\") :\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        eval_output_dir = args.output_dir\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    # eval_batch_size for selection must be 1 to handle variable number of candidates\n",
    "    if args.task == \"selection\":\n",
    "        args.eval_batch_size = 1\n",
    "    else:\n",
    "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        collate_fn=eval_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    data_infos = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=args.local_rank not in [-1, 0]):\n",
    "        with torch.no_grad():\n",
    "            loss, lm_logits, mc_logits, mc_labels = run_batch_fn(args, model, batch)\n",
    "            if args.task == \"selection\":\n",
    "                data_infos.append(batch[-1])\n",
    "            all_preds.append(mc_logits.detach().cpu().numpy())\n",
    "            all_labels.append(mc_labels.detach().cpu().numpy())\n",
    "            eval_loss += loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    if args.task.lower() == \"generation\":\n",
    "        perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "        result = {\"perplexity\": perplexity, \"loss\": eval_loss}\n",
    "    elif args.task.lower() == \"selection\":\n",
    "        all_labels = np.array(all_labels).reshape(-1)\n",
    "        all_pred_ids = np.array([np.argmax(logits) for logits in all_preds])\n",
    "        accuracy = np.sum(all_pred_ids == all_labels) / len(all_labels)\n",
    "        logger.info(\"Avg. # of candidates: %f\", sum([len(arr[0]) for arr in all_preds]) / len(all_preds))\n",
    "        result = {\"loss\": eval_loss, \"accuracy\": accuracy}\n",
    "        if args.output_file:\n",
    "            sorted_pred_ids = [np.argsort(logits.squeeze())[::-1] for logits in all_preds]\n",
    "            write_selection_preds(eval_dataset.dataset_walker, args.output_file, data_infos, sorted_pred_ids, topk=5)\n",
    "    else:\n",
    "        raise ValueError(\"args.task not in ['generation', 'selection', 'detection'], got %s\" % args.task)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            logger.info(\"***** Eval results %s *****\" % desc)\n",
    "            writer.write(\"***** Eval results %s *****\\n\" % desc)\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train or evaluate for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_generation(args, model, batch):\n",
    "    batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "    input_ids, token_type_ids, lm_labels = batch\n",
    "    model_outputs = model(input_ids=input_ids, token_type_ids=None, labels=lm_labels)\n",
    "    loss = model_outputs[0]\n",
    "    lm_logits = model_outputs[1]\n",
    "    return loss, lm_logits, torch.tensor([]), torch.tensor([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627f075ade70412fa720af2c55be35a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4796.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de406aa017344870afae75a7a6d1dcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=50.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Epoch:  50%|█████     | 1/2 [09:54<09:54, 594.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67671c750cbd4d018086904ba8d3a2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4796.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3825e9ae6bbf470ba6cda362b498f3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=50.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [19:50<00:00, 595.09s/it]\n"
     ]
    }
   ],
   "source": [
    "global_step, tr_loss = train(args, train_dataset, eval_dataset, model, tokenizer, \n",
    "                              run_batch_generation, run_batch_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.per_gpu_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.labels_file = \"data/val/labels.json\"\n",
    "args.output_file = \"pred/val/generation.json\"\n",
    "args.checkpoint = \"./outputs/generation/checkpoint-2398\"\n",
    "args.generation_params_file = \"configs/generation_eval_params.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/val/labels.json\n",
      "pred/val/generation.json\n",
      "configs/generation_eval_params.json\n"
     ]
    }
   ],
   "source": [
    "print(args.labels_file)\n",
    "print(args.output_file)\n",
    "print(args.generation_params_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.params_file, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    args = vars(args)\n",
    "    update_additional_params(params, args)\n",
    "    args.update(params)\n",
    "    if len(args[\"generation_params_file\"]) > 0:\n",
    "        with open(args[\"generation_params_file\"]) as fg:\n",
    "            generation_params = json.load(fg)\n",
    "        args.update(generation_params)\n",
    "    args = Namespace(**args)\n",
    "\n",
    "args.params = params # used for saving checkpoints\n",
    "dataset_args = Namespace(**args.dataset_args)\n",
    "dataset_args.local_rank = args.local_rank\n",
    "dataset_args.task = args.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/val/labels.json\n",
      "pred/val/generation.json\n",
      "configs/generation_eval_params.json\n"
     ]
    }
   ],
   "source": [
    "print(args.labels_file)\n",
    "print(args.output_file)\n",
    "print(args.generation_params_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from utils.metrics import (\n",
    "    UnigramMetric, NGramDiversity,\n",
    "    CorpusNGramDiversity,\n",
    "    BLEU, METEOR, ROUGE\n",
    ")\n",
    "\n",
    "from utils.data import write_generation_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: joblib in /home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied: regex in /home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: click in /home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages (from nltk) (7.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jennybae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/generation'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pred/val/generation.json'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/val/labels.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.labels_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_generation_sample(args, model, batch, dataset):\n",
    "    special_tokens_ids = args.tokenizer.convert_tokens_to_ids(dataset.SPECIAL_TOKENS_VALUES)\n",
    "    current_output = []\n",
    "\n",
    "    example = batch[0]\n",
    "    knowledge, history = example[\"knowledge\"], example[\"history\"]\n",
    "    response_text = example[\"response_text\"]\n",
    "    dialog_id = example[\"dialog_id\"]\n",
    "\n",
    "    for i in range(args.max_length):\n",
    "        instance, sequence = dataset.build_input_from_segments(\n",
    "            knowledge, history, current_output, with_eos=False\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0)\n",
    "\n",
    "        model_outputs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        logits = model_outputs[0]\n",
    "\n",
    "        logits = logits[0, -1, :] / args.temperature\n",
    "        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
    "        if i < args.min_length and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                if probs.max().item() == 1:\n",
    "                    logger.warning(\"Warning: model generating special token with probability 1! Breaking...\")\n",
    "                    break\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "    \n",
    "    return current_output, response_text, dialog_id\n",
    "\n",
    "def top_filtering(logits, top_k=0, top_p=0.0, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (..., vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, eval_dataset, model, tokenizer, desc=\"\"):\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        eval_output_dir = args.output_dir\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=1, # only support batch_size=1 for sampling right now\n",
    "        collate_fn=eval_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    metrics = [\n",
    "        UnigramMetric(metric=\"recall\"),\n",
    "        UnigramMetric(metric=\"precision\"),\n",
    "        NGramDiversity(n=1),\n",
    "        NGramDiversity(n=2),\n",
    "        NGramDiversity(n=3),\n",
    "        NGramDiversity(n=4),\n",
    "        CorpusNGramDiversity(n=1),\n",
    "        CorpusNGramDiversity(n=2),\n",
    "        CorpusNGramDiversity(n=3),\n",
    "        CorpusNGramDiversity(n=4),\n",
    "        BLEU(),\n",
    "        METEOR(),\n",
    "        ROUGE()\n",
    "    ]\n",
    "\n",
    "    args.tokenizer = tokenizer\n",
    "    all_output_texts = []\n",
    "    dialog_ids = []\n",
    "    do_evaluate = False\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=args.local_rank not in [-1, 0]):\n",
    "        with torch.no_grad():\n",
    "            sampled_output_ids, ground_truth, dialog_id = run_batch_generation_sample(args, model, batch, eval_dataset)\n",
    "            sampled_output_text = tokenizer.decode(sampled_output_ids, skip_special_tokens=True)\n",
    "            all_output_texts.append(sampled_output_text)\n",
    "            dialog_ids.append(dialog_id)\n",
    "        if ground_truth.strip() != \"\":\n",
    "            do_evaluate = True\n",
    "            for metric in metrics:\n",
    "                metric.update((sampled_output_text, ground_truth))\n",
    "\n",
    "    if args.output_file:\n",
    "        write_generation_preds(eval_dataset.dataset_walker, args.output_file, dialog_ids, all_output_texts)\n",
    "\n",
    "    result = dict()\n",
    "    if do_evaluate and args.local_rank in [-1, 0]:\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            logger.info(\"***** Eval results %s *****\" % desc)\n",
    "            writer.write(\"***** Eval results %s *****\\n\" % desc)\n",
    "            for metric in metrics:\n",
    "                name = metric.name()\n",
    "                score = metric.compute()\n",
    "                result[name] = score\n",
    "                logger.info(\"  %s = %s\", name, str(score))\n",
    "                writer.write(\"%s = %s\\n\" % (name, str(score)))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50264, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(args.checkpoint)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d217c5d404432799efe8cfcb871103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f040b6ae446b3b6c4e14985d415b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db9b85c541440e18f723062ff65f34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=200.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jennybae/miniconda3/envs/dstc9/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = ResponseGenerationEvalDataset(dataset_args, tokenizer, split_type=\"val\", \n",
    "                                             labels_file=args.labels_file)\n",
    "result = evaluate(args, eval_dataset, model, tokenizer, \n",
    "                  desc=args.eval_desc or \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UnigramRecall': 0.42050354337099854,\n",
       " 'UnigramPrecision': 0.43602368513901346,\n",
       " '1GramDiversity': 0.9508670903476464,\n",
       " '2GramDiversity': 0.9473561393083778,\n",
       " '3GramDiversity': 0.8984176590912578,\n",
       " '4GramDiversity': 0.8481108636368873,\n",
       " 'Corpus1GramDiversity': 0.0983125458547322,\n",
       " 'Corpus2GramDiversity': 0.2619222303741746,\n",
       " 'Corpus3GramDiversity': 0.3690388848129127,\n",
       " 'Corpus4GramDiversity': 0.4326241134751773,\n",
       " 'BLEU': 0.08675389797650139,\n",
       " 'METEOR': 0.3761632690299168,\n",
       " 'ROUGE': 0.2895436928846583}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"scripts/scores.py\", line 1, in <module>\r\n",
      "    from utils.dataset_walker import DatasetWalker\r\n",
      "ModuleNotFoundError: No module named 'utils'\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/scores.py --dataset val --dataroot data --outfile pred/val/generation.json --scorefile pred/val/generation.score.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
